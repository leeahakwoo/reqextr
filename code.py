# app.py

import streamlit as st
import re
import pandas as pd
from typing import List, Dict, Tuple, Any
import docx
from docx.document import Document
from docx.table import Table
from docx.text.paragraph import Paragraph
import io

# --- ë¡œì§ í´ë˜ìŠ¤ 1: ì¶”ì¶œê¸° (ê²¬ê³ ì„± ê°•í™”) ---
class StreamlitDocxExtractor:
    def __init__(self, business_code: str = "MFDS"):
        self.business_code = business_code

    def _generate_id(self, req_id: str, sequence: int) -> str:
        req_type_code = "F" if req_id.startswith("FUN") else "Q"
        id_num_match = re.search(r'\d+', req_id)
        id_num = id_num_match.group(0) if id_num_match else "000"
        return f"REQ-{self.business_code}-{req_type_code}-{id_num}{sequence:03d}"

    def _get_all_paragraphs_in_order(self, doc: Document) -> List[Paragraph]:
        all_paragraphs = []
        for block in doc.element.body:
            if isinstance(block, docx.oxml.text.paragraph.CT_P):
                all_paragraphs.append(Paragraph(block, doc))
            elif isinstance(block, docx.oxml.table.CT_Tbl):
                table = Table(block, doc)
                for row in table.rows:
                    for cell in row.cells:
                        all_paragraphs.extend(cell.paragraphs)
        return all_paragraphs

    def _get_indentation_level(self, p: Paragraph) -> int:
        indent = p.paragraph_format.left_indent
        return indent.pt if indent else 0

    def _parse_details_from_paragraphs(self, paragraphs: List[Paragraph], req_id: str, level1_bullets: str, level2_bullets: str) -> List[Dict]:
        final_requirements = []
        bfn_seq_counter = 1
        group_stack = []
        # [ìˆ˜ì •] SyntaxWarning í•´ê²° ë° ê³µë°± í—ˆìš©ì„ ìœ„í•´ Raw String(r'')ê³¼ \s* ì‚¬ìš©
        l1_pattern = re.compile(rf'^\s*[{re.escape(level1_bullets)}]')
        l2_pattern = re.compile(rf'^\s*[{re.escape(level2_bullets)}]')

        for p in paragraphs:
            # strip() í•˜ì§€ ì•Šì€ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ë¸”ë¦¿ ì—¬ë¶€ ë¨¼ì € í™•ì¸
            raw_text = p.text
            if not raw_text.strip(): continue
            
            is_level1 = bool(l1_pattern.search(raw_text))
            is_level2 = bool(l2_pattern.search(raw_text))
            current_level = self._get_indentation_level(p)
            
            if not is_level1 and not is_level2: continue

            while group_stack and current_level < group_stack[-1]['level']:
                group_stack.pop()

            # [ìˆ˜ì •] ë¸”ë¦¿ê³¼ ì•ë’¤ ê³µë°±ì„ ì œê±°í•œ ìˆœìˆ˜ ë‚´ìš© ì¶”ì¶œ
            clean_line = re.sub(rf'^\s*[{re.escape(level1_bullets + level2_bullets)}]+\s*', '', raw_text).strip()
            if not clean_line: continue

            if is_level1:
                while group_stack and current_level <= group_stack[-1]['level']:
                    group_stack.pop()
                group_stack.append({'title': clean_line, 'level': current_level})
            elif is_level2 and group_stack:
                # 2ì°¨ ë¸”ë¦¿ì€ ìŠ¤íƒì— ì¶”ê°€í•˜ì§€ ì•Šê³  ë°”ë¡œ ì‚¬ìš©
                pass

            if group_stack:
                group_name = group_stack[0]['title']
                detail_content = clean_line
                
                is_duplicate = any(req['ìš”êµ¬ì‚¬í•­ ê·¸ë£¹'] == group_name and req['ì„¸ë¶€ ìš”êµ¬ì‚¬í•­ ë‚´ìš©'] == detail_content for req in final_requirements)
                if not is_duplicate:
                    final_requirements.append({
                        'ìš”êµ¬ì‚¬í•­ ê·¸ë£¹': group_name, 'ì„¸ë¶€ ìš”êµ¬ì‚¬í•­ ë‚´ìš©': detail_content,
                        'ì„¸ë¶€ ìš”êµ¬ì‚¬í•­ ID': self._generate_id(req_id, bfn_seq_counter)
                    })
                    bfn_seq_counter += 1
        return final_requirements

    def process(self, docx_file: io.BytesIO, level1_bullets: str, level2_bullets: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        doc = docx.Document(docx_file)
        all_paragraphs = self._get_all_paragraphs_in_order(doc)
        
        block_markers = [i for i, p in enumerate(all_paragraphs) if 'ìš”êµ¬ì‚¬í•­ ë¶„ë¥˜' in p.text]
        if not block_markers: return pd.DataFrame(), pd.DataFrame()

        all_requirements = []
        high_level_reqs = [] 
        for i, start_index in enumerate(block_markers):
            end_index = block_markers[i+1] if i + 1 < len(block_markers) else len(all_paragraphs)
            block_paragraphs = all_paragraphs[start_index:end_index]
            block_text = "\n".join([p.text for p in block_paragraphs])
            
            # [ê°œì„ ] ì½œë¡ (:) ìœ ë¬´, ê³µë°± ë“± ë‹¤ì–‘í•œ í¬ë§·ì— ëŒ€ì‘í•˜ë„ë¡ ì •ê·œì‹ ê°•í™”
            req_id_match = re.search(r'ìš”êµ¬ì‚¬í•­\s*ê³ ìœ ë²ˆí˜¸\s*[:]?\s*([A-Z]{3}-\d{3})', block_text)
            req_name_match = re.search(r'ìš”êµ¬ì‚¬í•­\s*ëª…ì¹­\s*[:]?\s*(.+?)(?:\n|$)', block_text)

            if not req_id_match or not req_name_match: continue
            req_id, req_name = req_id_match.group(1).strip(), req_name_match.group(1).strip()
            
            details_start_index_offset = next((j + 1 for j, p in enumerate(block_paragraphs) if 'ì„¸ë¶€ë‚´ìš©' in p.text), -1)
            
            if details_start_index_offset != -1:
                details_paragraphs = block_paragraphs[details_start_index_offset:]
                parsed_reqs = self._parse_details_from_paragraphs(details_paragraphs, req_id, level1_bullets, level2_bullets)
                
                for req in parsed_reqs:
                    req['ìš”êµ¬ì‚¬í•­ ID (RFP ì›ì²œ)'] = req_id
                    req['ìš”êµ¬ì‚¬í•­ ëª…ì¹­ (RFP ì›ì²œ)'] = req_name
                all_requirements.extend(parsed_reqs)
                
                if parsed_reqs:
                    # 'ìš”êµ¬ì‚¬í•­ ê·¸ë£¹'ì˜ ê³ ìœ í•œ ê°œìˆ˜ë¥¼ ì„¸ì–´ ìš”ì•½ ì •ë³´ ìƒì„±
                    unique_groups = pd.Series([r['ìš”êµ¬ì‚¬í•­ ê·¸ë£¹'] for r in parsed_reqs]).nunique()
                    high_level_reqs.append({
                        'ìš”êµ¬ì‚¬í•­ ID': req_id, 'ìš”êµ¬ì‚¬í•­ ëª…ì¹­': req_name,
                        'ìœ í˜•': 'ê¸°ëŠ¥' if req_id.startswith('FUN') else 'ë¹„ê¸°ëŠ¥',
                        'ê¸°ëŠ¥ ê·¸ë£¹ ìˆ˜': unique_groups,
                        'ì´ ì„¸ë¶€ í•­ëª© ìˆ˜': len(parsed_reqs)
                    })
        summary_df = pd.DataFrame(high_level_reqs)
        details_df = pd.DataFrame(all_requirements)
        return summary_df, details_df

# --- ë¡œì§ í´ë˜ìŠ¤ 2: ì œê³µí•´ì£¼ì‹  í‘œì¤€í™”ê¸° (ë‚´ìš© ìƒì„± ë‹´ë‹¹) ---
class RFPStandardizer:
    def __init__(self):
        self.standard_format = {'header_levels': {1: '###', 2: '####', 3: '#####'}, 'requirement_prefix': 'FUR-', 'priority_mapping': {'í•„ìˆ˜': 'Essential', 'ê¶Œì¥': 'Recommended', 'ì„ íƒ': 'Optional'}}
    def standardize_requirements(self, raw_requirements: List[Dict]) -> List[Dict]:
        return [self._convert_to_standard_format(req, idx) for idx, req in enumerate(raw_requirements, 1)]
    def _convert_to_standard_format(self, requirement: Dict, index: int) -> Dict:
        req_id = f"FUR-{index:03d}"
        sub_requirements = [{'id': f"{req_id}-{sub_idx:03d}", 'name': self._clean_text(detail), 'description': self._generate_description(detail), 'input_info': self._generate_input_info(detail), 'output_info': self._generate_output_info(detail), 'processing_conditions': self._generate_processing_conditions(detail), 'deliverables': self._generate_deliverables(detail)} for sub_idx, detail in enumerate(requirement.get('details', []), 1)]
        return {'id': req_id, 'category': requirement.get('category', 'ë¯¸ë¶„ë¥˜'), 'name': requirement.get('name', '').strip(), 'priority': self._map_priority(requirement.get('priority', 'í•„ìˆ˜')), 'department': requirement.get('department', 'ì „ì‚°ê´€ë¦¬ë¶€ì„œ'), 'sub_requirements': sub_requirements, 'summary': self._generate_summary(requirement)}
    def _clean_text(self, text: str) -> str:
        if not text: return ""
        return re.sub(r'\s+', ' ', re.sub(r'^[-*>\s]+', '', text)).strip()
    def _map_priority(self, priority: str) -> str:
        return self.standard_format['priority_mapping'].get(priority, 'Essential')
    def _generate_description(self, detail: str) -> str:
        d = {'í”„ë¡œê·¸ë¨ê´€ë¦¬': 'ì‹œìŠ¤í…œ ë‚´ í”„ë¡œê·¸ë¨ ë“±ë¡, ìˆ˜ì •, ì‚­ì œ ê´€ë¦¬', 'ì‚¬ìš©ìê´€ë¦¬': 'ì‹œìŠ¤í…œ ì‚¬ìš©ì ê³„ì • ìƒì„±, ìˆ˜ì •, ì‚­ì œ ë° ê¶Œí•œ ë¶€ì—¬', 'ê¶Œí•œê´€ë¦¬': 'ì‚¬ìš©ìë³„, ê·¸ë£¹ë³„ ì‹œìŠ¤í…œ ì ‘ê·¼ ê¶Œí•œ ì„¤ì •', 'íœ´ì¼ê´€ë¦¬': 'ê³µíœ´ì¼, ì„ì‹œíœ´ì¼, ëŒ€ì²´ê³µíœ´ì¼ ë“±ë¡ ë° ê´€ë¦¬', 'ë¡œê·¸ì¸ì´ë ¥': 'ì‚¬ìš©ì ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ ì´ë ¥ ì¶”ì  ë° ê´€ë¦¬', 'TABLEì •ë³´': 'ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” êµ¬ì¡° ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬', 'SMSì „ì†¡ë¬¸êµ¬ê´€ë¦¬': 'SMS ë°œì†¡ìš© í…œí”Œë¦¿ ë“±ë¡ ë° ê´€ë¦¬', 'í•™ì‚¬ë ¥ê´€ë¦¬': 'í•™ê¸°ë³„ í•™ì‚¬ì¼ì • ë“±ë¡ ë° ê´€ë¦¬', 'ê³µì§€ì‚¬í•­ê´€ë¦¬': 'ì „ì²´ ê³µì§€ì‚¬í•­ ë“±ë¡, ìˆ˜ì •, ì‚­ì œ ê´€ë¦¬', 'ê³µí†µì½”ë“œê´€ë¦¬': 'ì‹œìŠ¤í…œ ì „ë°˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê³µí†µì½”ë“œ ê´€ë¦¬', 'í•™ê³¼ë¶€ì„œê´€ë¦¬': 'ëŒ€í•™ ë‚´ í•™ê³¼ ë° ë¶€ì„œ ì¡°ì§ë„ ê´€ë¦¬', 'ëŒ€í•™ë²•ì¸ê´€ë¦¬': 'ëŒ€í•™ ë²•ì¸ ì •ë³´ ë° ê´€ë ¨ ë°ì´í„° ê´€ë¦¬', 'ë¶€ì„œë³€ê²½ê´€ë¦¬': 'ì¡°ì§ ë³€ê²½ ì´ë ¥ ë° ë¶€ì„œ ê°œí¸ ê´€ë¦¬'}
        clean_detail = self._clean_text(detail)
        return next((desc for keyword, desc in d.items() if keyword in clean_detail), f"{clean_detail} ê¸°ëŠ¥ êµ¬í˜„")
    def _generate_input_info(self, detail: str) -> str:
        d = {'í”„ë¡œê·¸ë¨ê´€ë¦¬': 'í”„ë¡œê·¸ë¨ID, í”„ë¡œê·¸ë¨ëª…, ì„¤ëª…, ìƒíƒœ', 'ì‚¬ìš©ìê´€ë¦¬': 'ì‚¬ìš©ìID, ì„±ëª…, ë¶€ì„œ, ì§ê¸‰, ì—°ë½ì²˜', 'ê¶Œí•œê´€ë¦¬': 'ì‚¬ìš©ìID/ê·¸ë£¹ID, ë©”ë‰´ë³„ ê¶Œí•œ(ì¡°íšŒ/ë“±ë¡/ìˆ˜ì •/ì‚­ì œ)', 'íœ´ì¼ê´€ë¦¬': 'íœ´ì¼ë‚ ì§œ, íœ´ì¼ëª…, íœ´ì¼êµ¬ë¶„, ë°˜ë³µì—¬ë¶€', 'ë¡œê·¸ì¸ì´ë ¥': 'ì‚¬ìš©ìID, ì ‘ì†IP, ì ‘ì†ì‹œê°„, ë¸Œë¼ìš°ì €ì •ë³´', 'TABLEì •ë³´': 'í…Œì´ë¸”ëª…, ì»¬ëŸ¼ì •ë³´, ì¸ë±ìŠ¤, ì œì•½ì¡°ê±´', 'SMSì „ì†¡ë¬¸êµ¬ê´€ë¦¬': 'í…œí”Œë¦¿ID, ì œëª©, ë‚´ìš©, ë°œì†¡ì¡°ê±´', 'í•™ì‚¬ë ¥ê´€ë¦¬': 'í•™ë…„ë„, í•™ê¸°, ì¼ì •êµ¬ë¶„, ì‹œì‘ì¼, ì¢…ë£Œì¼', 'ê³µì§€ì‚¬í•­ê´€ë¦¬': 'ì œëª©, ë‚´ìš©, ì²¨ë¶€íŒŒì¼, ê³µì§€ê¸°ê°„, ëŒ€ìƒì', 'ê³µí†µì½”ë“œê´€ë¦¬': 'ì½”ë“œê·¸ë£¹, ì½”ë“œê°’, ì½”ë“œëª…, ì •ë ¬ìˆœì„œ, ì‚¬ìš©ì—¬ë¶€'}
        clean_detail = self._clean_text(detail)
        return next((info for keyword, info in d.items() if keyword in clean_detail), f"{clean_detail} ê´€ë ¨ ì…ë ¥ ë°ì´í„°")
    def _generate_output_info(self, detail: str) -> str:
        d = {'í”„ë¡œê·¸ë¨ê´€ë¦¬': 'í”„ë¡œê·¸ë¨ ëª©ë¡, ìƒì„¸ì •ë³´', 'ì‚¬ìš©ìê´€ë¦¬': 'ì‚¬ìš©ì ëª©ë¡, ê¶Œí•œ í˜„í™©', 'ê¶Œí•œê´€ë¦¬': 'ê¶Œí•œ ë§¤íŠ¸ë¦­ìŠ¤, ê¶Œí•œ ë³€ê²½ ì´ë ¥', 'íœ´ì¼ê´€ë¦¬': 'íœ´ì¼ ë‹¬ë ¥, íœ´ì¼ ëª©ë¡', 'ë¡œê·¸ì¸ì´ë ¥': 'ë¡œê·¸ì¸ ì´ë ¥ ì¡°íšŒ, í†µê³„ ë¦¬í¬íŠ¸', 'TABLEì •ë³´': 'í…Œì´ë¸” ëª…ì„¸ì„œ, ERD', 'SMSì „ì†¡ë¬¸êµ¬ê´€ë¦¬': 'í…œí”Œë¦¿ ëª©ë¡, ë°œì†¡ ì´ë ¥', 'í•™ì‚¬ë ¥ê´€ë¦¬': 'í•™ì‚¬ë ¥ ë‹¬ë ¥, í•™ì‚¬ì¼ì •í‘œ', 'ê³µì§€ì‚¬í•­ê´€ë¦¬': 'ê³µì§€ì‚¬í•­ ëª©ë¡, ì¡°íšŒ í†µê³„', 'ê³µí†µì½”ë“œê´€ë¦¬': 'ì½”ë“œ ëª©ë¡, ì½”ë“œ ì²´ê³„ë„'}
        clean_detail = self._clean_text(detail)
        return next((info for keyword, info in d.items() if keyword in clean_detail), f"{clean_detail} ê´€ë ¨ ì¶œë ¥ ë°ì´í„°")
    def _generate_processing_conditions(self, detail: str) -> str:
        d = {'í”„ë¡œê·¸ë¨ê´€ë¦¬': 'ê´€ë¦¬ì ê¶Œí•œ í•„ìš”', 'ì‚¬ìš©ìê´€ë¦¬': 'ì‹œìŠ¤í…œ ê´€ë¦¬ì ê¶Œí•œ í•„ìš”', 'ê¶Œí•œê´€ë¦¬': 'ìµœê³ ê´€ë¦¬ì ê¶Œí•œ í•„ìš”', 'íœ´ì¼ê´€ë¦¬': 'ë§¤ë…„ íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸', 'ë¡œê·¸ì¸ì´ë ¥': 'ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘, 6ê°œì›” ì´ìƒ ë³´ê´€', 'TABLEì •ë³´': 'DB ê´€ë¦¬ì ê¶Œí•œ í•„ìš”', 'SMSì „ì†¡ë¬¸êµ¬ê´€ë¦¬': 'í†µì‹ ì‚¬ ê·œê²© ì¤€ìˆ˜', 'í•™ì‚¬ë ¥ê´€ë¦¬': 'í•™ì‚¬ê´€ë¦¬ ê¶Œí•œ í•„ìš”', 'ê³µì§€ì‚¬í•­ê´€ë¦¬': 'ë¶€ì„œë³„ ê³µì§€ ê¶Œí•œ ì°¨ë“± ì ìš©', 'ê³µí†µì½”ë“œê´€ë¦¬': 'ì½”ë“œ ì¤‘ë³µ ë°©ì§€, í‘œì¤€í™” ì¤€ìˆ˜'}
        clean_detail = self._clean_text(detail)
        return next((cond for keyword, cond in d.items() if keyword in clean_detail), f"{clean_detail} ê´€ë ¨ ì²˜ë¦¬ ì¡°ê±´ ì ìš©")
    def _generate_deliverables(self, detail: str) -> str:
        d = {'í”„ë¡œê·¸ë¨ê´€ë¦¬': 'í”„ë¡œê·¸ë¨ ê´€ë¦¬ëŒ€ì¥', 'ì‚¬ìš©ìê´€ë¦¬': 'ì‚¬ìš©ì ë“±ë¡ëŒ€ì¥, ê¶Œí•œ ë¶€ì—¬ ë‚´ì—­', 'ê¶Œí•œê´€ë¦¬': 'ê¶Œí•œ ê´€ë¦¬ëŒ€ì¥', 'íœ´ì¼ê´€ë¦¬': 'ì—°ê°„ íœ´ì¼ ê³„íší‘œ', 'ë¡œê·¸ì¸ì´ë ¥': 'ì ‘ì† ì´ë ¥ ë³´ê³ ì„œ', 'TABLEì •ë³´': 'ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ì„œ', 'SMSì „ì†¡ë¬¸êµ¬ê´€ë¦¬': 'SMS ë°œì†¡ í†µê³„', 'í•™ì‚¬ë ¥ê´€ë¦¬': 'ì—°ê°„ í•™ì‚¬ì¼ì •í‘œ', 'ê³µì§€ì‚¬í•­ê´€ë¦¬': 'ê³µì§€ì‚¬í•­ ë°œí–‰ ëŒ€ì¥', 'ê³µí†µì½”ë“œê´€ë¦¬': 'ê³µí†µì½”ë“œ ê´€ë¦¬ëŒ€ì¥'}
        clean_detail = self._clean_text(detail)
        return next((item for keyword, item in d.items() if keyword in clean_detail), f"{clean_detail} ê´€ë ¨ ì‚°ì¶œë¬¼")
    def _generate_summary(self, requirement: Dict) -> str:
        return f"{requirement.get('name', '')} ì˜ì—­ì˜ {len(requirement.get('details', []))}ê°œ ì„¸ë¶€ ê¸°ëŠ¥ êµ¬í˜„"
    def export_to_markdown(self, standardized_requirements: List[Dict], project_name: str) -> str:
        md = [f"# ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ (ì‹ì•½ì²˜ í‘œì¤€ ê¸°ì¤€)\n", "## 1. ì‚¬ì—… ê°œìš”", f"- **ì‚¬ì—…ëª…**: {project_name}", "- **ì‚¬ì—… ë¶„ì•¼**: [ì‚¬ì—… ë¶„ì•¼ ì…ë ¥]", "- **ë‚©ê¸°**: [ë³„ë„ ëª…ì‹œ]\n", "## 2. ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ (Functional Requirements)\n"]
        for req in standardized_requirements:
            md.extend([f"### {req['id']} {req['name']}", f"**ë¶„ë¥˜**: {req['category']}", f"**ìš°ì„ ìˆœìœ„**: {req['priority']}", f"**ë‹´ë‹¹ë¶€ì„œ**: {req['department']}", f"**ìš”ì•½**: {req['summary']}\n"])
            if req['sub_requirements']:
                for sub_req in req['sub_requirements']:
                    md.extend([f"#### {sub_req['id']} {sub_req['name']}", f"- **ê¸°ëŠ¥ì„¤ëª…**: {sub_req['description']}", f"- **ì…ë ¥ì •ë³´**: {sub_req['input_info']}", f"- **ì¶œë ¥ì •ë³´**: {sub_req['output_info']}", f"- **ì²˜ë¦¬ì¡°ê±´**: {sub_req['processing_conditions']}", f"- **ì‚°ì¶œì •ë³´**: {sub_req['deliverables']}\n"])
        return "\n".join(md)

# --- ë¡œì§ í´ë˜ìŠ¤ 3: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì§€íœ˜ì (Orchestrator) ---
class RFPAnalysisOrchestrator:
    def __init__(self, business_code: str):
        self.extractor = StreamlitDocxExtractor(business_code)
        self.standardizer = RFPStandardizer()
        self.business_code = business_code

    def _prepare_for_standardization(self, details_df: pd.DataFrame) -> List[Dict]:
        if details_df.empty: return []
        raw_requirements = []
        grouped = details_df.groupby('ìš”êµ¬ì‚¬í•­ ID (RFP ì›ì²œ)')
        
        for name, group in grouped:
            # 1ì°¨, 2ì°¨ ë¸”ë¦¿ ë‚´ìš©ì„ ëª¨ë‘ Standardizerì— ì „ë‹¬
            details_list = group['ì„¸ë¶€ ìš”êµ¬ì‚¬í•­ ë‚´ìš©'].unique().tolist()
            req_dict = {
                'category': 'ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­' if name.startswith('FUN') else 'ë¹„ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­',
                'name': group['ìš”êµ¬ì‚¬í•­ ëª…ì¹­ (RFP ì›ì²œ)'].iloc[0],
                'priority': 'í•„ìˆ˜', 'details': details_list }
            raw_requirements.append(req_dict)
        return raw_requirements

    def run(self, docx_file: io.BytesIO, level1_bullets: str, level2_bullets: str) -> Tuple[str, pd.DataFrame, pd.DataFrame]:
        summary_df, details_df = self.extractor.process(docx_file, level1_bullets, level2_bullets)
        if details_df.empty:
            return "ì˜¤ë¥˜: ë¬¸ì„œì—ì„œ ìš”êµ¬ì‚¬í•­ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 'ìš”êµ¬ì‚¬í•­ ë¶„ë¥˜' ë˜ëŠ” 'ì„¸ë¶€ë‚´ìš©' í‚¤ì›Œë“œì™€ ë¸”ë¦¿ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.", pd.DataFrame(), pd.DataFrame()
        raw_reqs = self._prepare_for_standardization(details_df)
        standardized_data = self.standardizer.standardize_requirements(raw_reqs)
        project_name = f"{self.business_code} ì •ë³´ì‹œìŠ¤í…œ êµ¬ì¶•"
        markdown_output = self.standardizer.export_to_markdown(standardized_data, project_name)
        return markdown_output, summary_df, details_df

# --- Streamlit UI êµ¬ì„± ---
def main():
    st.set_page_config(page_title="ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ìë™ ìƒì„±ê¸°", layout="wide")
    st.title("ğŸ“‘ DOCX ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ ìë™ í‘œì¤€í™” ë° ìƒì„±")
    st.markdown("""
    **ì–´ë–¤ í˜•ì‹ì˜ ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ(.docx)ë“  ì—…ë¡œë“œë§Œ í•˜ì„¸ìš”!**
    1.  ë¬¸ì„œ ë‚´ì˜ ìš”êµ¬ì‚¬í•­ì„ **ìë™ìœ¼ë¡œ ì¶”ì¶œ**í•©ë‹ˆë‹¤.
    2.  ì¶”ì¶œëœ ë‚´ìš©ì„ **'ì‹ì•½ì²˜(MFDS) RFP í‘œì¤€'**ì— ë§ì¶° ë³€í™˜í•˜ê³  ë‚´ìš©ì„ ë³´ê°•í•©ë‹ˆë‹¤.
    3.  ì™„ì„±ëœ **í‘œì¤€ ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ**ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """)

    with st.sidebar:
        st.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
        business_code = st.text_input("ì‚¬ì—… ì½”ë“œ", value="MFDS", help="ì‚¬ì—…ì„ ì‹ë³„í•˜ëŠ” ê³ ìœ  ì½”ë“œì…ë‹ˆë‹¤.")
        st.markdown("---")
        st.subheader("1ë‹¨ê³„: ì›ë³¸ ë¬¸ì„œ ë¶„ì„ ì„¤ì •")
        st.info("ë¬¸ì„œì— ì‚¬ìš©ëœ ëŒ€í‘œ ë¸”ë¦¿ ë¬¸ìë¥¼ ì…ë ¥í•˜ì„¸ìš”. ë¸”ë¦¿ ì•/ë’¤ ê³µë°±ì´ë‚˜ ì•½ê°„ì˜ ë³€í˜•ì€ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        level1_bullets = st.text_input("ê¸°ëŠ¥ ê·¸ë£¹ ë¸”ë¦¿(1ì°¨)", value="*â—¦â—‹â€¢", help="ì˜ˆ: * íšŒì›ê°€ì… ê¸°ëŠ¥")
        level2_bullets = st.text_input("ì„¸ë¶€ í•­ëª© ë¸”ë¦¿(2ì°¨)", value="-Â·â–´", help="ì˜ˆ: - ì´ë©”ì¼ë¡œ ê°€ì…")

    uploaded_file = st.file_uploader("ë¶„ì„í•  .docx íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["docx"])

    if uploaded_file is not None:
        try:
            orchestrator = RFPAnalysisOrchestrator(business_code=business_code)
            
            with st.spinner("ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ë° í‘œì¤€ ëª…ì„¸ì„œ ìƒì„± ì¤‘..."):
                # ì—…ë¡œë“œëœ íŒŒì¼ì„ BytesIOë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬
                file_bytes = io.BytesIO(uploaded_file.getvalue())
                markdown_result, summary_df, details_df = orchestrator.run(
                    file_bytes, level1_bullets, level2_bullets
                )

            if "ì˜¤ë¥˜:" in markdown_result:
                st.error(markdown_result)
            else:
                st.success("âœ… í‘œì¤€ ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ ìƒì„±ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")
                
                st.subheader("ğŸ“„ ìµœì¢… ì‚°ì¶œë¬¼: í‘œì¤€ ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ")
                st.markdown(markdown_result, help="ì•„ë˜ ë‚´ìš©ì€ ì‹ì•½ì²˜ í‘œì¤€ì— ë”°ë¼ ìë™ ìƒì„±ëœ ëª…ì„¸ì„œì…ë‹ˆë‹¤.")
                
                st.download_button(
                    label="ğŸ“¥ ëª…ì„¸ì„œ ë‹¤ìš´ë¡œë“œ (.md íŒŒì¼)",
                    data=markdown_result.encode('utf-8'),
                    file_name=f"Standardized_Requirements_{business_code}.md",
                    mime="text/markdown",
                )

                st.markdown("---")

                with st.expander("ì›ë³¸ ì¶”ì¶œ ë°ì´í„° ë³´ê¸° (1ë‹¨ê³„ ê²°ê³¼)"):
                    st.subheader("ğŸ“Š ìƒìœ„ ê¸°ëŠ¥ ìš”ì•½")
                    st.dataframe(summary_df)
                    st.subheader("ğŸ“‹ ì›ë³¸ ì„¸ë¶€ ìš”êµ¬ì‚¬í•­ ëª©ë¡")
                    st.dataframe(details_df)

        except Exception as e:
            st.error(f"âŒ ë¶„ì„ ì¤‘ ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.exception(e)

if __name__ == '__main__':
    main()
